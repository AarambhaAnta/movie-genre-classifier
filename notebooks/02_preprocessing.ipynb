{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook:02_preprocession.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import swifter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure src module can be found\n",
    "sys.path.append('../src')\n",
    "from data_loader import load_data\n",
    "from preprocess import clean_text_parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data from: ../data/raw/train_data.txt\n",
      "Data loaded successfully from ../data/raw/train_data.txt\n",
      "Shape of dataset: (54214, 4)\n"
     ]
    }
   ],
   "source": [
    "# Load train data\n",
    "train_path = \"../data/raw/train_data.txt\"\n",
    "\n",
    "print(f\"Loading training data from: {train_path}\")\n",
    "train_df = load_data(train_path, is_train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/adityakumar/nltk_data...\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/adityakumar/nltk_data...\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/adityakumar/nltk_data...\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/adityakumar/nltk_data...\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/adityakumar/nltk_data...\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/adityakumar/nltk_data...\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/adityakumar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date![nltk_data]   Package punkt is already up-to-date!\n",
      "\n",
      "[nltk_data]   Package punkt is already up-to-date![nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/adityakumar/nltk_data...\n",
      "\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/adityakumar/nltk_data...\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/adityakumar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date![nltk_data]   Package stopwords is already up-to-date!\n",
      "\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/adityakumar/nltk_data...[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/adityakumar/nltk_data...\n",
      "\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/adityakumar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/adityakumar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/adityakumar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/adityakumar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/adityakumar/nltk_data...\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/adityakumar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/adityakumar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/adityakumar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/adityakumar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/adityakumar/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data]   Package wordnet is already up-to-date![nltk_data]   Package wordnet is already up-to-date!\n",
      "\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/adityakumar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/adityakumar/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Apply parallelized text cleaning\n",
    "train_df['Cleaned_Description'] = clean_text_parallel(train_df['Description'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved to: ../data/processed/cleaned_train_data.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(54214, 5)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save cleaned text data\n",
    "cleaned_path = \"../data/processed/cleaned_train_data.csv\"\n",
    "train_df.to_csv(cleaned_path, index=False)\n",
    "print(f\"Cleaned data saved to: {cleaned_path}\")\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Genre\n",
       "drama          13613\n",
       "documentary    13096\n",
       "comedy          7447\n",
       "short           5073\n",
       "horror          2204\n",
       "thriller        1591\n",
       "action          1315\n",
       "western         1032\n",
       "reality-tv       884\n",
       "family           784\n",
       "adventure        775\n",
       "music            731\n",
       "romance          672\n",
       "sci-fi           647\n",
       "adult            590\n",
       "crime            505\n",
       "animation        498\n",
       "sport            432\n",
       "talk-show        391\n",
       "fantasy          323\n",
       "mystery          319\n",
       "musical          277\n",
       "biography        265\n",
       "history          243\n",
       "game-show        194\n",
       "news             181\n",
       "war              132\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['Genre'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply TF-IDF Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # Limiting vocab size for efficiency\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(train_df['Cleaned_Description'])\n",
    "\n",
    "tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaron</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abc</th>\n",
       "      <th>abducted</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>aboard</th>\n",
       "      <th>abortion</th>\n",
       "      <th>abroad</th>\n",
       "      <th>...</th>\n",
       "      <th>youngster</th>\n",
       "      <th>youre</th>\n",
       "      <th>youth</th>\n",
       "      <th>youtube</th>\n",
       "      <th>youve</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zero</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.21397</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 5000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aaron  abandon  abandoned  abc  abducted  ability  able  aboard  abortion  \\\n",
       "0    0.0      0.0        0.0  0.0       0.0      0.0   0.0     0.0       0.0   \n",
       "1    0.0      0.0        0.0  0.0       0.0      0.0   0.0     0.0       0.0   \n",
       "2    0.0      0.0        0.0  0.0       0.0      0.0   0.0     0.0       0.0   \n",
       "3    0.0      0.0        0.0  0.0       0.0      0.0   0.0     0.0       0.0   \n",
       "4    0.0      0.0        0.0  0.0       0.0      0.0   0.0     0.0       0.0   \n",
       "\n",
       "   abroad  ...  youngster  youre  youth  youtube  youve  zealand     zero  \\\n",
       "0     0.0  ...        0.0    0.0    0.0      0.0    0.0      0.0  0.00000   \n",
       "1     0.0  ...        0.0    0.0    0.0      0.0    0.0      0.0  0.00000   \n",
       "2     0.0  ...        0.0    0.0    0.0      0.0    0.0      0.0  0.00000   \n",
       "3     0.0  ...        0.0    0.0    0.0      0.0    0.0      0.0  0.00000   \n",
       "4     0.0  ...        0.0    0.0    0.0      0.0    0.0      0.0  0.21397   \n",
       "\n",
       "   zombie  zone  zoo  \n",
       "0     0.0   0.0  0.0  \n",
       "1     0.0   0.0  0.0  \n",
       "2     0.0   0.0  0.0  \n",
       "3     0.0   0.0  0.0  \n",
       "4     0.0   0.0  0.0  \n",
       "\n",
       "[5 rows x 5000 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed TF-IDF data saved to: ../data/processed/train_data_tfidf.csv\n"
     ]
    }
   ],
   "source": [
    "# Save TF-IDF features\n",
    "processed_path = \"../data/processed/train_data_tfidf.csv\"\n",
    "tfidf_df.to_csv(processed_path, index=False)\n",
    "print(f\"Processed TF-IDF data saved to: {processed_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF vectorizer saved to: ../models/saved_models/tfidf_vectorizer.pkl\n"
     ]
    }
   ],
   "source": [
    "# Save TF-IDF vectorizer for later use\n",
    "import joblib\n",
    "vectorizer_path = \"../models/saved_models/tfidf_vectorizer.pkl\"\n",
    "joblib.dump(tfidf_vectorizer, vectorizer_path)\n",
    "print(f\"TF-IDF vectorizer saved to: {vectorizer_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54214, 5)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting data into train and test sets\n",
    "y = train_df['Genre']  # Assuming the target column is 'Genre'\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_df, y, test_size=0.2, random_state=42)\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43371, 5000)\n",
      "(10843, 5000)\n",
      "(43371,)\n",
      "(10843,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-test split data saved.\n"
     ]
    }
   ],
   "source": [
    "# Save train-test split\n",
    "X_train.to_csv(\"../data/processed/X_train.csv\", index=False)\n",
    "X_test.to_csv(\"../data/processed/X_test.csv\", index=False)\n",
    "y_train.to_csv(\"../data/processed/y_train.csv\", index=False)\n",
    "y_test.to_csv(\"../data/processed/y_test.csv\", index=False)\n",
    "print(\"Train-test split data saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (genre-classifier)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
